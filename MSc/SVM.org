#+TITLE: Support Vector Machines
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \usepackage{tikz}
#+LaTeX_HEADER: \usepackage{amsmath}
#+LaTeX_HEADER: \usepackage{amssymb}
#+LaTeX_HEADER: \usepackage{isomath}
#+LaTeX_HEADER: \newcommand \E {\mathop{\mbox{\ensuremath{\mathbb{E}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Var {\mathop{\mbox{\ensuremath{\mathbb{V}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Bias {\mathop{\mbox{\ensuremath{\mathbb{B}}}}\nolimits}
#+LaTeX_HEADER: \newcommand\ind[1]{\mathop{\mbox{\ensuremath{\mathbb{I}}}}\left\{#1\right\}}
#+LaTeX_HEADER: \renewcommand \Pr {\mathop{\mbox{\ensuremath{\mathbb{P}}}}\nolimits}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmax}{arg\,max}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmin}{arg\,min}
#+LaTeX_HEADER: \DeclareMathOperator*{\sgn}{sgn}
#+LaTeX_HEADER: \newcommand \defn {\mathrel{\triangleq}}
#+LaTeX_HEADER: \newcommand \Reals {\mathbb{R}}
#+LaTeX_HEADER: \newcommand \bW {\matrixsym{W}}
#+LaTeX_HEADER: \newcommand \bw {\vectorsym{w}}
#+LaTeX_HEADER: \newcommand \wi {\vectorsym{w}_i}
#+LaTeX_HEADER: \newcommand \wij {w_{i,j}}
#+LaTeX_HEADER: \newcommand \bA {\matrixsym{A}}
#+LaTeX_HEADER: \newcommand \ai {\vectorsym{a}_i}
#+LaTeX_HEADER: \newcommand \aij {a_{i,j}}
#+LaTeX_HEADER: \newcommand \bx {\vectorsym{x}}
#+LaTeX_HEADER: \newcommand \bz {\vectorsym{z}}
#+LaTeX_HEADER: \newcommand \param {\beta}
#+LaTeX_HEADER: \newcommand \vparam {\vectorsym{\beta}}
#+LaTeX_HEADER: \newcommand \Params {\mathcal{B}}
#+LaTeX_HEADER: \newcommand \vmult {\vectorsym{\lambda}}
#+LaTeX_HEADER: \newcommand \mult {\lambda}
#+LaTeX_HEADER: \newcommand \Ber {\textrm{Bernoulli}}
#+LaTeX_HEADER: \newcommand \Beta {\textrm{Beta}}
#+LaTeX_HEADER: \newcommand \Normal {\textrm{Normal}}
#+LaTeX_CLASS_OPTIONS: [smaller]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:3
* Background
** Maximal margin
*** Hyperplane
If $x \in \Reals^n$, then an affine subspace of dimension $n-1$ is a hyperplane.
**** Definition
A hyperplane in $\Reals^n$ is the set of points satisfying
\[
\{\bx : \param_0 + \vparam^\top \bx =0 \}
\]
**** Separating hyperplane
Consider a dataset $(\bx_t, y_t)$ of points with $y_t \in \{-1, 1\}$. If 
\[
(\param_0 + \vparam^\top \bx_t) y_t \qquad \forall t
\]
then the hyperplane separates the dataset.
*** The maximal margin hyperplane
**** The margin
For any a dataset $(\bx_t, y_t)$, and hyperplane we can define the margin
\[
M = \min_t (\param_0 + \vparam^\top \bx_t) y_t 
\]
as the minimum distance between the hyperplane and a correctly classied point.
**** The maximal margin hyperplane
Similarly, there is some $\param_0, \vparam$ that achieves the maximum separation:
\[
\max_{\param_0, \vparam} \min_t (\param_0 + \vparam^\top \bx_t) y_t 
\]
*** The maximum margin classifier
**** The optimisation problem
We can write the problem like this
\begin{align}
\max_{\param_0, \vparam, M} & M \tag{maximise the margin}\\
\textrm{s.t.} & \|\vparam\| = 1 \tag{invariance} \\
 & y_t(\param_0 + \vparam^\top \bx_t) \geq M && \forall t \in [T] \tag{margin for all examples}.
\end{align}
And we can divide by $\|\vparam\|$ to remove the norm constraint:
\[
 y_t(\param_0 + \vparam^\top \bx_t) \geq M \|\vparam\|, \qquad \forall t \in [T] 
\]
Setting $\|\vparam\| = 1/M$, we can rewrite this as
\begin{align*}
\min_{\param_0, \vparam} & \|\vparam\|^2\\
\textrm{s.t.} &  y_t(\param_0 + \vparam^\top \bx_t) \geq 1 && \forall t 
\end{align*}
*** Quadratic programming
**** A quadratic program has the form:
\begin{align*}
\min_\vparam &\|\vparam\|^2\\
 \textrm{s.t.} & \vparam^\top x_t \geq 1 \forall t.
\end{align*}
**** A constrained optimisation problem
\begin{align*}
\min_\vparam & f(\vparam)\\
 \textrm{s.t.} & g_i(\vparam) = 0 \forall i\\
 & h_i(\vparam) \geq 0 \forall i.
\end{align*}

We can use the \alert{Lagrange} method of multipliers to solve these problems.

*** Lagrange methods
**** Lagrange multipliers
For any local minimum $\vparam^*$, there is a unique vector $\vmult^* \in \Reals^n$ so that
\[
\nabla f(\vparam^*) + \sum_i \mult^*_i \nabla h_i(\vparam^*) = 0.
\]
**** The Lagrangian function
We first augment the original cost function to the \alert{Langrangian}
\[
L(\vparam, \vmult) = f(\vparam) + \sum_{i=1}^n \mult_i h_i(\vparam)
\]
For any local minimum $\vparam^*, \vmult^*$, 
$\nabla_\vparam L(\vparam^*, \vmult^*) = 0$, $\nabla_\vmult L(\vparam^*, \vmult^*) = 0$.

**** The Lagrange dual function
The dual function $D$ is always concave:
\[
D(\vmult) = \inf_{\vparam} L(\vparam, \vmult).
\]


* Support vectors 
** Support Vector Machines
*** Support vector machines
- Hyperplanes do not always work
- How about a non-linear boundary?
- Instead of mapping the inputs through a non-linearity, map inner products to a kernel
**** Kernelised linear functions
We can rewrite
\[
f(\bx) = \param_0 + \sum_{i=1}^n \param_i x_i
\]
in terms of a kernel $K : X \times X \to \Reals$:
\[
f(\bx) = \param_0 + \sum_{t=1}^T \alpha_t K(\bx, \bx_t), \qquad K(\bx, \bx_t) = \bx^\top \bx_t
\]
because we can find $\vectorsym{\alpha}$ so that 
\[
\sum_i  \sum_t \left(\alpha_t x_{t,i}\right) x_i = \sum_i \param_i x_i.
\]
In fact it is sufficient to have: $\sum_t \alpha_t x_{t,i} = \param_i$.
*** Kernels
**** Radial Basis Function
A simple type of non-linear layer in neural networks:
\[
f(x) = \sum_i \alpha_i K(\bx,  \vectorsym{c}_i),
\qquad
K(\bx, \vectorsym{c}_i) = \exp(-\|\bx - \vectorsym{c}_i\|^2),
\]
where $\vectorsym{c}_t$ are \alert{fixed centroids}
**** Kernels in SVMs
Instead of fixed kernels, use the \alert{training data}:
\[
f(x) = \sum_t \alpha_t K(\bx,  \bx_t),
\]
**** Some common kernel choices
- Linear: $K(\bx, \bx') = \bx^\top \bx$.
- RBFs: $K(\bx, \bx') = \exp(-\|bx - \bx'\|^2)$
- Polynomial: $K(\bx, \bx') = (1 + \bx^\top \bx)^d$.
*** Kernels as features*
Some kernels can be rewritten in terms of a feature mapping $\phi: X \to Z$
\[
K(\bx, \bx') = \phi^\top(\bx) \phi(\bx)
\]
- The mapping $\phi$ is implicit, and never computed.
- The dimension of $Z$ can be infinite.
- So-called Mercer kernels are symmetric: $K(\bx, \bx') = K(\bx', \bx)$.
**** Mercer kernels
$K : \Reals^n \times \Reals^n \to \Reals$  is a Mercer kernel, if for any $\{\bx_t : t \in [T]\}$, the kernel matrix
$\vectorsym{K} \in \Reals^{n \times n}$
\[
\vectorsym{K} \defn [K(\bx_i, \bx_j)]_{i,j \in [T]}
\]
is symmetric positive semi-definite, i.e.
\[
\bz^\top \vectorsym{K} \bz \geq 0 \qquad \forall \bz \in \Reals^n.
\]

