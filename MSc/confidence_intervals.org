#+TITLE: Confidence Intervals
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \usepackage{tikz}
#+LaTeX_HEADER: \usepackage{amsmath}
#+LaTeX_HEADER: \usepackage{amssymb}
#+LaTeX_HEADER: \usepackage{isomath}
#+LaTeX_HEADER: \newcommand \E {\mathop{\mbox{\ensuremath{\mathbb{E}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Var {\mathop{\mbox{\ensuremath{\mathbb{V}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Bias {\mathop{\mbox{\ensuremath{\mathbb{B}}}}\nolimits}
#+LaTeX_HEADER: \newcommand\ind[1]{\mathop{\mbox{\ensuremath{\mathbb{I}}}}\left\{#1\right\}}
#+LaTeX_HEADER: \renewcommand \Pr {\mathop{\mbox{\ensuremath{\mathbb{P}}}}\nolimits}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmax}{arg\,max}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmin}{arg\,min}
#+LaTeX_HEADER: \DeclareMathOperator*{\sgn}{sgn}
#+LaTeX_HEADER: \newcommand \defn {\mathrel{\triangleq}}
#+LaTeX_HEADER: \newcommand \Reals {\mathbb{R}}
#+LaTeX_HEADER: \newcommand \Param {B}
#+LaTeX_HEADER: \newcommand \param {\beta}
#+LaTeX_HEADER: \newcommand \vparam {\vectorsym{\beta}}
#+LaTeX_HEADER: \newcommand \mparam {\matrixsym{B}}
#+LaTeX_HEADER: \newcommand \bW {\matrixsym{W}}
#+LaTeX_HEADER: \newcommand \bw {\vectorsym{w}}
#+LaTeX_HEADER: \newcommand \wi {\vectorsym{w}_i}
#+LaTeX_HEADER: \newcommand \wij {w_{i,j}}
#+LaTeX_HEADER: \newcommand \bA {\matrixsym{A}}
#+LaTeX_HEADER: \newcommand \ai {\vectorsym{a}_i}
#+LaTeX_HEADER: \newcommand \aij {a_{i,j}}
#+LaTeX_HEADER: \newcommand \bx {\vectorsym{x}}
#+LaTeX_HEADER: \newcommand \by {\vectorsym{y}}
#+LaTeX_HEADER: \newcommand \bel {\beta}
#+LaTeX_HEADER: \newcommand \Ber {\textrm{Bernoulli}}
#+LaTeX_HEADER: \newcommand \Lapl {\textrm{Laplace}}
#+LaTeX_HEADER: \newcommand \Binom {\textrm{Binomial}}
#+LaTeX_HEADER: \newcommand \Beta {\textrm{Beta}}
#+LaTeX_HEADER: \newcommand \Normal {\textrm{Normal}}
#+LaTeX_HEADER: \newcommand \CH {\mathcal{H}}
#+LaTeX_CLASS_OPTIONS: [smaller]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:3

* Hypothesis testing
** Simple Hypothesis Tests
*** Simple hypothesis tests
- Consider $n$ hypotheses, $H_1, \ldots, H_n \in \CH$
- Each hypothesis corresponds to a model $P(x | H_i)$ giving a probability value to every possible data $x \in X$.
- Given specific data $x$, we want to select the most likely model. 
**** Maximum Likelihood 
Pick the model with the highest likelihood:
- $\hat{H} = \argmax_{H_i} P(x | H_i)$
**** Maximum A Posteriori
- Given prior $P(H_i)$
- Pick $\hat{H} = \argmax_{H_i} P(H_i | x)$
- We use Bayes's theorem to calculate the posterior $P(H_i | x)$.
- When $P(H_i)$ is uniform, it is the same as maximum likelihood.

*** The Theorem of Bayes
- Given some probability space $(P, \Omega, \Sigma)$.
- $P$ is a probability measure on $\Omega$
- $\Omega$ is the outcome space.
- $\Sigma$ is a collection of subsets of $\Omega$, corresponding to events.
- Let $\{H_i\}$ be a partition of $\Omega$, i.e.
\[
H_i \cup H_j = \emptyset ~ \forall i \neq j, \qquad  \bigcup_i H_i = \Omega.
\] 
Then, for any event $A \in \Sigma$, $A \subset \Omega$, 
\[
P(H_i | A) = \frac{P(A | H_i) P(H_i)}{\sum_j P(A | H_j) P(H_j)}
\]
*** Proof of Bayes's theorem
Note that $P(H_i \cap A) = P(H_i | A) P(A) = P(A | H_i) P(H_i)$. Rearranging,
\[
 P(H_i | A) = \frac{P(A | H_i) P(H_i)}{P(A)}.
\]
Since $\{H_i\}$ is a partition,
\[
P(A) = P\left(\bigcup_i A \cap H_i\right) = \sum_i P(A \cap H_i) = \sum_i P(A | H_i) P(H_i)
\]
**** Extensions
- We can use any non-negative scoring function $f_h(x)$:
\[
P(h | x) = \frac{f_h(x) P(h)}{\sum_{h' \in \CH} f_{h'}(x) P(h')}
\]
- For infinite $\CH$ we can use this notation:
\[
P(B | x) = \frac{\int_{B} f_h(x) dP(h)}{\int_{\CH} f_h(x) dP(h)}, \qquad B \subset \CH.
\]
*** Null Hypothesis Tests
- Consider a model $H_0$ such that $P(x | H_0)$ is known.
- We need to compare against an *unknown* alternative.
- We calculate a *statistic* $s : X \to \Reals$ to partition $X$ in $S_0, S_1$ i.e.
\[
S_0 = \{x : s(x) \leq \theta\},
\qquad
S_1 = \{x : s(x) > \theta\}
\]
- Then $P(S_0 | H_0) = 1 - \alpha$, $P(S_1 | H_0) = \alpha$ for some $\alpha$
- We tune $\theta$ to achieve the desired $\alpha$.
- If $x \in S_0$, we accept $H_0$, otherwise we reject it.
**** Example statistics
- Likelihood test: $s(x) = P(x|H_0)$, 
- Mean test: $s(x) = |x - \E[x | H_0]|^2$.
*** Likelihood test
- We can use $s(x) = P(x | H_0)$.
- Now we can choose a threshold $\theta$ so that:
\[
S_1 = \{x : s(x) \geq \theta\}
\]
**** Example: Laplace distributiotan
- Density: $f(x | \mu, \lambda) = \frac{1}{2\lambda} e^{-\frac{1}{\lambda} |x - \mu|}$
- $H_0$: $x \sim \Lapl(0, 1)$.
- $f(x | 0, 1) \geq \theta$ means $|x| \leq \ln(1/2 \theta)$. So 
\[
P(S_1 | H_0) =
\int_{-infty}{^-\ln(1/2\theta)} e^{-x} dx
=
1/2\theta.
\]
- Consequently,  $\theta = 1/2(1 -  \alpha)$, i.e. we accept $H_0$ if $|x| \leq \ln (4 - 4\alpha)$

*** Bernoulli test                                                  :example:
- $H_0$: The coin tosses are fair
- Then the probability of any sequence $x = x_1, \ldots, x_T$ is $2^{-T}$.
- The expected number of heads is $T/2$.
- Statistic $s(x) = \sum_t x_t$.
- Select interval $S = [c T, (1 - c) T]$.
- There is some $c \in [0,1/2]$ so that $P(S | H_0) = 1 - \alpha$
- To calculate $c$ we can use the inverse CDF of $s$.


*** p values
**** How to use $p$ values
- First select a significance threshold $\alpha$.
- Collect the data, obtain the $p$ value
- If $p \leq \alpha$, reject the null hypothesis $H_0$.
- This ensures that, if $H_0$ is true, the probability of rejecting it is exactly $\alpha$! (Because $p$ is uniform in $[0,1]$ under $H_0$)

**** Problems with $p$ values
- They do not measure quality of fit on the data.
- Not robust to model misspecification. 
- They ignore effect sizes. 
- They do not consider prior information. 
- They do not represent the probability of having made an error
- The null-rejection error probability is the same irrespective of the amount of data (by design).

* Mean estimation
** Estimating a mean
*** Mean estimation
- Data $D =x_1, \ldots, x_T$
- i.i.d samples $x_t \sim P$
- Expectation $\E_P(x_t) = \mu$, 
- Empirical mean:
\[
\hat{\mu}(D) = \frac{1}{T} \sum_{t=1}^T x_t.
\]
**** The error of the empirical mean
Since the data $D$ is random, what is the probability that our estimate is far away from $\mu$? 
\[
\Pr[|\hat{\mu}(D) - \mu| > \epsilon] \leq \delta.
\]
This means that the probability that our error is larger than $\epsilon$ is at most $\delta$, with s $\epsilon, \delta > 0$.

**** Two methods:
- Distribution-specific confidence intervals
- Concentration inequalities

*** Distribution-specific intervals
**** Bernoulli 
If $x_t \sim \Ber(\mu)$, then the distribution of $\hat{\mu}$ is given by
the Binomial distribution.

**** Binomial
Let $n_t = \sum_{i=1}^t x_i$, where $x_t \sim \Ber(\mu)$. Then $n_t$ has a binomial distribution with parameter $\mu$ and $t$ trials, i.e. $n_t \sim \Binom(\mu, t)$, and its probability function is
\[
\Pr(n_t = k) = \binom{t}{k} \mu^k (1 - \mu)^{1 - k} 
\]

** Concentration inequalities
*** Markov's Inequality
If $x \geq 0$
\[
\Pr(x \geq u) \leq \frac{\E[x]}{u}
\]
**** Proof
\begin{align}
\E[x]
& =  \int_0^\infty x p(x) dx\\
& =  \int_0^u x p(x) dx + \int_u^\infty x p(x) dx\\
& \geq  \int_u^\infty u p(x) dx\\
& =  u \Pr(x \geq u)
\end{align}
*** Chernoff bound
\[
\Pr(x - \mu \geq u) 
=
\Pr(e^{\lambda(x - \mu)} \geq e^{\lambda u}) 
\leq 
\frac{\E[e^{\lambda(x - \mu)}]}{e^{\lambda u}}
\]
- This follows directly from Markov's inequality.
- Tuning $\lambda$ gives us the tightest bound.
*** Normal tail bound
**** Moment generating function
If $x \sim \Normal(\mu, \sigma^2)$ then
\[
\E[e^{\lambda x}]
= 
e^{\mu \lambda + \sigma^2 \lambda^2 / 2}
\]
**** Proof
\begin{align}
\E[e^{\lambda x}]
&=
\frac{1}{\sigma \sqrt{2 \pi}}
\int_{-\infty}^\infty
e^{\lambda x}
e^{-\frac{|x - \mu|^2}{2 \sigma^2}}
dx
&=
\frac{1}{\sigma \sqrt{2 \pi}}
\int_{-\infty}^\infty
e^{\lambda x -\frac{|x - \mu|^2}{2 \sigma^2}}
dx
\\
&=
\frac{1}{\sigma \sqrt{2 \pi}}
\int_{-\infty}^\infty
e^{\lambda (\sqrt{2} \sigma y + \mu) - y^2}
dy
\end{align}
where $y = (x-\mu)/(\sqrt{2} \sigma)$, so $x = \sqrt{2} \sigma y + \mu$.
**** Normal tail bound
If $x_t \sim \Normal(\mu, 1)$, then
\[
\Pr(|x_t - \mu| > \epsilon) \leq 2 e^{- \epsilon^2/2}
\]
*** Normal bound
- $\hat{\mu} \sim \Normal(\mu, 1/T)$.
- For any $c > 0$,  $\Var[c x] = c \Var[x] \Rightarrow T \hat{\mu} \sim \Normal(T \mu, 1)$. So:
\begin{align}
\Pr(|T\hat{\mu} - T\mu| \geq \epsilon) 
&\leq 2 e^{- \epsilon^2/2}
&&\textrm{from the tail bound}
\\
\Pr(|\hat{\mu} - \mu| \geq \epsilon/T)
&\leq 2 e^{- \epsilon^2/2}
&&\textrm{as $a \geq b \Leftrightarrow c a \geq c b$ for $c > 0$}
\\
\Pr(|\hat{\mu} - \mu| \geq u)
&\leq 2 e^{- T^2u^2/2}
&& \textrm{where $u = \epsilon / T$}
\end{align}

*** Hoeffding bound

**** Hoeffding Inequality
For any sequence of independent (but not identical) rv's $x_1, \ldots, x_T$, with $x_t \in [a_t, b_t]$,
and consider the sum $s_T = \sum_{t=1}^T x_t$, which is also random. Then
\[
\Pr(s_T \geq \E[s_t] + \epsilon) \leq \exp\left(- \epsilon^2 / \sum_{t}(b_t - a_t)^2\right).
\]
**** Corollary
For any sequence of independent rv's $x_1, \ldots, x_T$, with $x_t \in [0, 1]$,
with expectation $\E[x_t] = \mu$
it holds for the empirical mean $z_T = \frac{1}{T} \sum_{t=1}^T x_t$:
\[
\Pr(|\mu - z_T| \geq \epsilon) \leq 2 \exp\left( -2n \epsilon^2\right)
\]

    
* Exercises

** Conditional probability
*** Bayesian Reasoning
  You are tested for COVID are found negative. The doctor says that the probability of a false positive (i.e. that the probability that the test is positive if you do not have COVID) is $1/10$ and the probability of a negative test if you have COVID is $1/5$.  The prevalence of COVID in the population in the population $1/10$. What is the probability that you actually have COVID?
** Hypothesis testing
*** Exercise
**** A statistical test
Consider the null hypothesis $H_0$ that $x_t \sim \Ber(1/2)$ and the sample mean $\hat{\mu_T} = \frac{1}{T} \sum_{t=1}^T x_t$. The probability of making an error of more than $\epsilon$ is
\[
1 - \sum_{k={T \epsilon}}{T\epsilon}
\]


