#+TITLE: Generative Modelling
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \usepackage{tikz}
#+LaTeX_HEADER: \usepackage{amsmath}
#+LaTeX_HEADER: \usepackage{amssymb}
#+LaTeX_HEADER: \usepackage{isomath}
#+LaTeX_HEADER: \newcommand \E {\mathop{\mbox{\ensuremath{\mathbb{E}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Var {\mathop{\mbox{\ensuremath{\mathbb{V}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Bias {\mathop{\mbox{\ensuremath{\mathbb{B}}}}\nolimits}
#+LaTeX_HEADER: \newcommand\ind[1]{\mathop{\mbox{\ensuremath{\mathbb{I}}}}\left\{#1\right\}}
#+LaTeX_HEADER: \renewcommand \Pr {\mathop{\mbox{\ensuremath{\mathbb{P}}}}\nolimits}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmax}{arg\,max}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmin}{arg\,min}
#+LaTeX_HEADER: \DeclareMathOperator*{\sgn}{sgn}
#+LaTeX_HEADER: \newcommand \defn {\mathrel{\triangleq}}
#+LaTeX_HEADER: \newcommand \Reals {\mathbb{R}}
#+LaTeX_HEADER: \newcommand \Param {\Theta}
#+LaTeX_HEADER: \newcommand \param {\theta}
#+LaTeX_HEADER: \newcommand \vparam {\vectorsym{\theta}}
#+LaTeX_HEADER: \newcommand \mparam {\matrixsym{\Theta}}
#+LaTeX_HEADER: \newcommand \bW {\matrixsym{W}}
#+LaTeX_HEADER: \newcommand \bw {\vectorsym{w}}
#+LaTeX_HEADER: \newcommand \wi {\vectorsym{w}_i}
#+LaTeX_HEADER: \newcommand \wij {w_{i,j}}
#+LaTeX_HEADER: \newcommand \bA {\matrixsym{A}}
#+LaTeX_HEADER: \newcommand \ai {\vectorsym{a}_i}
#+LaTeX_HEADER: \newcommand \aij {a_{i,j}}
#+LaTeX_HEADER: \newcommand \bx {\vectorsym{x}}
#+LaTeX_HEADER: \newcommand \bel {\beta}
#+LaTeX_HEADER: \newcommand \Ber {\textrm{Bernoulli}}
#+LaTeX_HEADER: \newcommand \Mult {\textrm{Multinomial}}
#+LaTeX_HEADER: \newcommand \Beta {\textrm{Beta}}
#+LaTeX_HEADER: \newcommand \Normal {\textrm{Normal}}
#+LaTeX_HEADER: \tikzstyle{utility}=[diamond,draw=black,draw=blue!50,fill=blue!10,inner sep=0mm, minimum size=8mm]
#+LaTeX_HEADER: \tikzstyle{select}=[rectangle,draw=black,draw=blue!50,fill=blue!10,inner sep=0mm, minimum size=6mm]
#+LaTeX_HEADER: \tikzstyle{hidden}=[dashed,draw=black,fill=red!10]
#+LaTeX_HEADER: \tikzstyle{RV}=[circle,draw=black,draw=blue!50,fill=blue!10,inner sep=0mm, minimum size=6mm]
#+LaTeX_CLASS_OPTIONS: [smaller]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:3

* Classification
** Classification: Generative modelling
   #+TOC: headlines [currentsection,hideothersubsections]
*** Generative modelling
**** general idea
- Data $(x,y)$.
- Need to model $P(y | x)$.
- Model the complete data distribution: $P(x | y)$, $P(x)$, $P(y)$.
- Calculate \(  P(y | x) = \frac{P(x | y) P(x)}{P(y)}. \)
**** Examples
- Naive Bayes classifier
- Gaussian Mixture Classifier
**** Modelling the data distribution
- Need to estimate the density $P(x | y)$ for each class $y$.
- Need to estimate $P(y)$
*** The basic graphical model

**** A discriminative classification model
Here $P(y|x)$ is given directly.
\begin{tikzpicture}
      \node[RV] at (2,0) (x) {$x$};
      \node[RV] at (0,0) (y) {$y$};
      \draw[->] (x) to (y);
\end{tikzpicture}

**** A generative classification model
Here $P(y | x) = P(x | y) P(y) / P(x)$.
\begin{tikzpicture}
      \node[RV] at (2,0) (x) {$x$};
      \node[RV] at (0,0) (y) {$y$};
      \draw[->] (y) to (x);
\end{tikzpicture}
**** A generative  model
Here we just have $P(x)$.
\begin{tikzpicture}
      \node[RV] at (2,0) (x) {$x$};
\end{tikzpicture}




*** Adding parameters to the graphical model
    
**** A Bernoulli RV
Here, $x | \theta \sim \Ber(\theta)$
\begin{tikzpicture}
\node[RV] at (2,0) (x) {$x$};
\node[RV,hidden] at (0,1) (mean) {$\theta$};
\draw[->] (mean) to (x);
\end{tikzpicture}

**** A normally distributed variable
Here $x  | \mu, \sigma \sim \Normal(\mu, \sigma^2)$
\begin{tikzpicture}
\node[RV] at (2,0) (x) {$x$};
\node[RV,hidden] at (0,1) (mean) {$\mu$};
\node[RV,hidden] at (1,1) (variance) {$\sigma$};
\draw[->] (mean) to (x);
\draw[->] (variance) to (x);
\end{tikzpicture}

*** Classification: Naive Bayes Classifier
- Data $(x,y)$
- $x \in X$
- $y \in Y \subset \mathbb{N}$, $N_i$: amount of data from class $i$.
  
**** Separately model each class
- Assume each class data comes from a different normal distribution
- $x | y = i \sim \Normal(\mu_i, \sigma_i I)$
- For each class, calculate
  - Empirical mean $\hat{\mu}_i = \sum_{t : y_t = i} x_t / N_i$
  - Empirical variance $\hat{\sigma}_i$.

**** Decision rule
Use Bayes's theorem:
\[
P(y | x) = P(x | y) P(y) / P(x),
\]
choosing the $y$ with largest posterior $P(y | x)$.
- $P(x | y = i) \propto \exp(- \|\hat{\mu}_i - x\|^2/\hat{\sigma}_i^2)$
*** Graphical model for the Naive Bayes Classifier
**** When $x \in \Reals$
Assume $k$ classes, then
- $\mu = (\mu_1, \ldots, \mu_k)$
- $\sigma = (\sigma_1, \ldots, \sigma_k)$
- \(\theta = (\theta_1, \ldots, \theta_k)\)
\begin{tikzpicture}
      \node[RV] at (2,0) (x) {$x$};
      \node[RV] at (0,0) (y) {$y$};
      \node[RV,hidden] at (2,1) (mean) {$\mu$};
      \node[RV,hidden] at (3,1) (variance) {$\sigma$};
      \node[RV,hidden] at (0,1) (choice) {$\theta$};
      \draw[->] (y) to (x);
      \draw[->] (mean) to (x);
      \draw[->] (variance) to (x);
      \draw[->] (choice) to (y);
\end{tikzpicture}
- $y \mid \theta \sim \Mult(\theta)$
- $x \mid y, \mu, \sigma \sim \Normal(\mu_y, \sigma^2_y)$
** Density estimation
*** General idea
**** Parametric models
- Fixed histograms
- Gaussian Mixtures
**** Non-parametric models
- Variable-bin histograms
- Infinite Gaussian Mixture Model
- Kernel methods

*** Histograms
**** Fixed histogram
- Hyper-Parameters: number of bins
- Parameters: Number of points in each bin.
**** Variable histogram
- Hyper-parameters: Rule for constructing bins
- Generally $\sqrt{n}$ points in each bin.

*** Gaussian Mixture Model
**** Hyperparameters:
- Number of Gaussian $k$.
**** Parameters:
- Multinomial distribution $\vparam$ over Gaussians
- For each Gaussian $i$, center $\mu_i$, covariance matrix $\Sigma_i$.
**** Model. For each point $x_t$:
- $c_t = i$ w.p. $\theta_i$
- $x_t | c_t = i \sim \Normal(\mu_i, \Sigma_i)$.
**** Algorithms:
- Expectation Maximisation
- Gradient Ascent
- Variational Bayesian Inference (with appropriate prior)

* Algorithms for latent variable models     

** Gradient algorithms
*** Gradient ascent
**** Objective function
$L(\theta) = P(x | \theta)$
**** Marginalisation over latent variable
$L(\theta) = \sum_z P(z, x | \theta) $
**** Gradient ascent
$\theta^{(n+1)} = \theta^{(n)} + \alpha \nabla_\theta L(\theta)$
**** Gradient calculation
Here we use the *log trick*: $\nabla \ln f(x) = \nabla f(x) / f(x)$.
\begin{align}
\nabla_\theta L(\theta)
& = \sum_z \nabla_\theta P(z, x \mid \theta) 
\\
&= \sum_z  P(z, x \mid \theta) \nabla_\theta \ln P(z, x \mid \theta)
\\
&= \sum_z  P(x \mid z, \theta)P(z \mid \theta) \nabla_\theta \ln P(z, x \mid \theta)
\\
&\approx \frac{1}{m} \sum_{i=1}^m P(x \mid z^{(i)}, \theta) \nabla_\theta \ln P(z^{(i)}, x \mid \theta)
&&z^{(i)} \sim P(z | \mid \theta)
\end{align}

** Expectation maximisation
*** A lower bound on the likelihood
\begin{align*}
\ln P(x | \alert{\theta})
& = \sum_z G(z) P(x | \theta)\\
& = \sum_z G(z) [\ln P(x, z | \theta) - \ln P(z | x, \theta)]
\\
& = \sum_z G(z) \ln P(x, z | \theta) - \sum_z G(z) \ln P(z | x, \theta)]
\\
& = \sum_z P(z | x, \theta^{(k)}) \ln P(x, z | \theta) - \sum_z P(z | x, \theta^{(k)}) \ln P(z | x, \theta)
\\
& \geq \sum_z P(z | x, \theta^{(k)}) \ln P(x, z | \alert{\theta}) - \sum_z P(z | x, \theta^{(k)}) \ln P(z | x, \theta^{(k)})
\\
& = Q(\alert{\theta} \mid \theta^{(k)}) +\mathbb{H}(z \mid x = x, \theta = \theta^{(k)})
\end{align*}
**** The Gibbs Inequality
$D_{KL}(P \|Q)  \geq 0$, or $\sum_x \ln P(x) P(x) \geq \sum_x \ln Q(x) P(x)$.
*** EM Algorithm (Dempster et al, 1977)
- Initial parameter $\vparam^{(0)}$, observed data $x$
- For $k=0, 1, \ldots$
-- Expectation step:
\[
Q(\alert{\vparam} \mid  \vparam^{(k)})
 \defn \E_{z \sim P(z | x, \vparam^{(k)})} [\ln P(x, z | \alert{\vparam}) ]
 = \sum_{z} [\ln P(x, z | \alert{\vparam})]  P(z  \mid x, \vparam^{(k)})
\]
-- Maximisation step:
\[
\vparam^{(k+1)} = \argmax_\vparam Q(\vparam, \vparam^{(k)}).
\]

See /Expectation-Maximization as lower bound maximization, Minka, 1998/

*** Minorise-Maximise
EM can be seen as a version of the minorise-maximise algorithm
- $f(\vparam)$: Target function to *maximise*
- $Q(\vparam | \vparam^{(k)})$: surrogate function
**** $Q$ Minorizes $f$
This means surrogate is always a lower bound so that
\[
f(\vparam) \geq Q(\vparam | \vparam^{(k)}),
\qquad
f(\vparam^{(k)}) \geq Q(\vparam^{(k)} | \vparam^{(k)}),
\]

**** Algorithm
- Calculate: $Q(\vparam | \vparam^{(k)}$
- Optimise: $\vparam^{(k+1)} = \argmax_\vparam Q(\vparam | \vparam^{(k)}$.



* Exercises
** Density estimation
*** GMM versus histogram
- Generate some data $x$ from an arbitrary distribution in $\Reals$.
- Fit the data with a histogram for varying numbers of bins
- Fit a GMM with varying numbers of Gaussians
- What is the best fit? How can you measure it?

** Classification
*** GMM Classifier :exercise:
**** Base class: sklearn GaussianMixtureModel
- /fit()/ only works for Density Estimaiton
- /predict()/ only predicts cluster labels
**** Problem
- Create a GMMClassifier class
- /fit()/ should take X, y, arguments
- /predict()/ should predict class labels
- Hint: Use /predict_proba()/ and multiple GMM models


