#+TITLE:  Reinforcement Learning
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \usepackage{tikz}
#+LaTeX_HEADER: \usepackage{amsmath}
#+LaTeX_HEADER: \usepackage{amssymb}
#+LaTeX_HEADER: \usepackage{isomath}
#+LaTeX_HEADER: \newcommand \E {\mathop{\mbox{\ensuremath{\mathbb{E}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Var {\mathop{\mbox{\ensuremath{\mathbb{V}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Bias {\mathop{\mbox{\ensuremath{\mathbb{B}}}}\nolimits}
#+LaTeX_HEADER: \newcommand\ind[1]{\mathop{\mbox{\ensuremath{\mathbb{I}}}}\left\{#1\right\}}
#+LaTeX_HEADER: \renewcommand \Pr {\mathop{\mbox{\ensuremath{\mathbb{P}}}}\nolimits}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmax}{arg\,max}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmin}{arg\,min}
#+LaTeX_HEADER: \DeclareMathOperator*{\sgn}{sgn}
#+LaTeX_HEADER: \newcommand \defn {\mathrel{\triangleq}}
#+LaTeX_HEADER: \newcommand \Reals {\mathbb{R}}
#+LaTeX_HEADER: \newcommand \Params {\Theta}
#+LaTeX_HEADER: \newcommand \param {\theta}
#+LaTeX_HEADER: \newcommand \vparam {\vectorsym{\theta}}
#+LaTeX_HEADER: \newcommand \mparam {\matrixsym{\Theta}}
#+LaTeX_HEADER: \newcommand \bW {\matrixsym{W}}
#+LaTeX_HEADER: \newcommand \bw {\vectorsym{w}}
#+LaTeX_HEADER: \newcommand \wi {\vectorsym{w}_i}
#+LaTeX_HEADER: \newcommand \wij {w_{i,j}}
#+LaTeX_HEADER: \newcommand \bA {\matrixsym{A}}
#+LaTeX_HEADER: \newcommand \ai {\vectorsym{a}_i}
#+LaTeX_HEADER: \newcommand \aij {a_{i,j}}
#+LaTeX_HEADER: \newcommand \bx {\vectorsym{x}}
#+LaTeX_HEADER: \newcommand \pol {\pi}
#+LaTeX_HEADER: \newcommand \Pols {\Pi}
#+LaTeX_HEADER: \newcommand \mdp {\mu}
#+LaTeX_HEADER: \newcommand \MDPs {\mathcal{M}}
#+LaTeX_HEADER: \newcommand \bel {\beta}
#+LaTeX_HEADER: \newcommand \Bels {\mathcal{B}}
#+LaTeX_HEADER: \newcommand \Unif {\textrm{Unif}}
#+LaTeX_HEADER: \newcommand \Ber {\textrm{Bernoulli}}
#+LaTeX_HEADER: \newcommand \Mult {\textrm{Mult}}
#+LaTeX_HEADER: \newcommand \Beta {\textrm{Beta}}
#+LaTeX_HEADER: \newcommand \Dir {\textrm{Dir}}
#+LaTeX_HEADER: \newcommand \Normal {\textrm{Normal}}
#+LaTeX_HEADER: \newcommand \Simplex {\mathbb{\Delta}}
#+LaTeX_HEADER: \newcommand \pn {\param^{(n)}}
#+LaTeX_HEADER: \newcommand \pnn {\param^{(n+1)}}
#+LaTeX_HEADER: \newcommand \pnp {\param^{(n-1)}}
#+LaTeX_HEADER: \usepackage[bbgreekl]{mathbbol}
#+LaTeX_HEADER: \tikzstyle{utility}=[diamond,draw=black,draw=blue!50,fill=blue!10,inner sep=0mm, minimum size=8mm]
#+LaTeX_HEADER: \tikzstyle{select}=[rectangle,draw=black,draw=blue!50,fill=blue!10,inner sep=0mm, minimum size=6mm]
#+LaTeX_HEADER: \tikzstyle{hidden}=[dashed,draw=black,fill=red!10]
#+LaTeX_HEADER: \tikzstyle{RV}=[circle,draw=black,draw=blue!50,fill=blue!10,inner sep=0mm, minimum size=6mm]
#+LaTeX_CLASS_OPTIONS: [smaller]
#+LATEX_HEADER: \RequirePackage{fancyvrb}
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:2
#+OPTIONS: toc:nil
* Bandit problems
** The multi-armed bandit (MAB) problem
- At time $t$:
- Select action $a_t \in A$
- Obtain reward $r_t \in \Reals$
*** Basic objective
Maximise total reward
\[
U = \sum_{t=1}^T r_t,
\]
where $T$ is the *horizon*. It may be unknown, or random.
*** Regret
We can instead minimise total regret
\[
L = \sum_{t=1}^T [r^*_t - r_t],
\]
where $r^*$ is the reward an oracle that knew the "best" arm would have obtained.

No let's make this more precise.

** The stochastic MAB
For each arm $i \in A$:
- $r_t \mid a_t = i \sim \mdp_i$ is the reward distribution
- $\rho_i \defn E_\mdp[r_t \mid a_t = i]$ the expected reward
- $\rho^* \defn \max_i \rho_i$.
*** Policy
The policy $\pol \in \Pols$ is a adaptive: $\pol(a_t \mid a_{t-1}, r_{t-1}, \ldots, a_1, r_1)$

*** Objective
Maximise expected total reward
\[
\E^\pol_\mdp[U] = \E^\pol_\mdp \left[\sum_{t=1}^T r_t \right]
\]
The total expected regret is
\[
\E^\pol_\mdp[L] = \E^\pol_\mdp \left[\sum_{t=1}^T \rho^* - \rho_t \right]
\]

** The horizon and regret
*** Discounted $T$
- $U = \sum_{t=1}^T \gamma^{t-1} r_t$
- Same as non-discounted with stopping probability $(1 - \gamma)$.

*** Arbitrary $T$
To compare algorithms, we use the notion of regret growth
- Linear regret: $L_T = O(T)$.  i.e. insufficient learning
- Sub-linear regret, e.g. $L_T = O(\sqrt{T})$ or $O(\ln T)$.

** Algorithms
*** \epsilon-greedy
- $\hat{\rho}_{i,t}$ is the average reward of arm $i$ at time $t$.
- w.p. $\epsilon$, $a_t \sim \Unif(A)$
- otherwise, $a_t = \argmax_{i}\hat{\rho}_{i,t}$, 
*** UCB 
- Play all arms once, and for $t > |A|$:
- $a_t = \argmax_i \hat{\rho}_{i,t} + \sqrt{2\ln(t)/n_{i,t}}$.
- $n_{i,t}$ is the number of times arm $i$ has been pulled until time $t$.
*** Thompson (posterior) sampling
Input: a prior $\bel_1$ over $\MDPs$.
- At time $t$:
- Sample from the posterior $\mdp^{(t)} \mid a_1, r_t, \ldots, a_{t-1}, r_{t-1} \sim \bel_t(\mdp)$
- Choose best action for sample: $a_t = \argmax_{i} \E_{\mdp^{(t)}}[r_t \mid a_t = i]$.
- Observe $r_t$.
- Calculate new posterior $\bel_{t+1}(\mdp) = \bel_t(\mdp \mid a_t, r_t)$.
** Other bandit problems

*** Adversarial bandits
- Rewards are arbitrary.
- Compare with best arm in hindsight.

*** Continuous bandits
- Actions $a_t \in \Reals^d$
- Example: Lipschitz bandits where $|\rho(a) - \rho(a')| \leq \|a - a'\|$.

*** Contextual bandits (in particular linear)
- Contexts $x_t \in \Reals^d$
- Unknown parameters $\theta_a \in \Reals^d$
- For the linear case $\rho(x, a) = x^\top \theta_a$.



* Markov decision processes
** The Markov decision process
Bandit problems are not dynamic. We can generalise reinforcement learning to dynamical systems through the MDP formalism:
- Action space $A$.
- State space $S$.
- Transition kernel $s_{t+1} = j \mid s_t = s, a_t = a \sim P_\mdp(j \mid s, a)$.
- Reward $r_t = \rho(s_t, a_t)$ (can also be random).
- Utility
\[
U_t = \sum_{k=t}^T r_t.
\]
** Value functions
*** The state value function
For any given MDP $\mdp$ and policy $\pol$ we define
\[
V^\pol_{\mdp, t}(s) \defn \E^\pol_{\mdp, t} \left[ U_t ~\middle|~ s_t = s \right]
\]
*** The state-action value function
\[
Q^\pol_{\mdp, t}(s, a) \defn \E^\pol_{\mdp, t} \left[ U_t ~\middle|~ s_t = s, a_t = a \right]
\]
*** The optimal value functions
For an optimal policy $\pol^*$
\[
V^*_{\mdp, t}(s) \defn V^{\pol^*}_{\mdp, t}(s) \geq V^\pol_{\mdp, t}(s),
\qquad
Q^*_{\mdp, t}(s,a) \defn Q^{\pol^*}_{\mdp, t}(s,a) \geq V^\pol_{\mdp, t}(s,a) 
\]
** The Bellman equations
*** State value function
\begin{align*}
V^\pol_{\mdp, t}(s)
& \defn \E^\pol_{\mdp, t}[U_{t}\mid s_t = s] \\
& = \E^\pol_{\mdp, t}[r_t + U_{t+1}\mid s_t = s] \\
& = \E^\pol_{\mdp}[r_t \mid s_t = s] + \E^\pol_{\mdp}[U_{t+1} \mid s_t = s]\\
& = \E^\pol_{\mdp}[r_t \mid s_t = s] + \sum_{j \in S} \E^\pol_{\mdp}[U_{t+1} \mid s_{t+1} = j] \Pr^\pol_\mdp(s_{t+1} = j \mid s_t = s)\\
& = \E^\pol_{\mdp}[r_t \mid s_t = s] + \sum_{j \in S} V^\pol_{\mdp, t+1}(j)  \Pr^\pol_\mdp(s_{t+1} = j \mid s_t = s)\\
& = \E^\pol_{\mdp}[r_t \mid s_t = s] + \sum_{j \in S} V^\pol_{\mdp, t+1}(j) \sum_{a \in A} P_\mdp(j \mid s, a) \pol(a_t \mid s_t).
\end{align*}
*** State-action value function
\begin{align*}
Q^\pol_{\mdp, t}(s)
&= [\rho(s,a) +  \sum_{j \in S} V^\pol_{\mdp, t+1}(j) P_\mdp(j \mid s, a)]
\end{align*}

** Optimal policies

*** Bellman optimality condition
The value function of the optimal policy satisfies this:
\begin{align*}
V^*_{\mdp, t}(s)
& = 
\max_{a}  [\rho(s,a) +  \sum_{j \in S} V^*_{\mdp, t+1}(j) P_\mdp(j \mid s, a)
\end{align*}
*** Dynamic programming 
To find $V^*, Q^*$, first initialise $V^*_{\mdp, T}(s) &= \max_a \rho(s,a)$. 
Then for $t = T-1, T-2, \ldots, 1$:
\begin{align*}
Q^*_{\mdp, t}(s,a) &= \rho(s,a) +  \sum_{j \in S} V^*_{\mdp, t+1}(j) P_\mdp(j \mid s, a).\\
V^*_{\mdp, t}(s) &= \max_a Q^*_{\mdp, t}(s,a).
\end{align*}
*** The optimal policy
The optimal policy is deterministic with:
\[
a_t = \argmax_a Q^*(s_t, a)
\]


* Reinforcement learning
** The Reinforcement Learning Problem
- Observe $x_t$
- Take action $a_t$
- Obtain reward $r_t$

*** Requirement for learning
- The model is not known
- Our policies must be *adaptive*

** Reinforcement learning settings
*** Fully observable, discrete Markov problems
- $x_t = s_t$, a Markovian state, $S, A$ finite.
- Optimal policies are Markov
- Can be solved efficiently with classical RL algorithms
*** Continuous Markov problems
- Requires function approximation
- Even when the model is known, hard to compute
*** Partially observable  problems
- Sufficient statistics are not finite


* External sources
** Further resources
- The Sutton/Barto RL intro book http://incompleteideas.net/book/the-book-2nd.html
- The Lattimore/Szepesvari bandit book https://tor-lattimore.com/downloads/book/book.pdf
- The Dimitrakakis/Ortner RL book https://ilias.unibe.ch/goto_ilias3_unibe_file_2946650_download.html 
- Reinforcement Learning Course at Neuchatel https://mcs.unibnf.ch/courses/reinforcement-learning-and-decision-making-under-uncertainty/
- OpenAI Gym https://github.com/openai/gym/

