#+TITLE: Latent variable models
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \usepackage{tikz}
#+LaTeX_HEADER: \usepackage{amsmath}
#+LaTeX_HEADER: \usepackage{amssymb}
#+LaTeX_HEADER: \usepackage{isomath}
#+LaTeX_HEADER: \newcommand \E {\mathop{\mbox{\ensuremath{\mathbb{E}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Var {\mathop{\mbox{\ensuremath{\mathbb{V}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Bias {\mathop{\mbox{\ensuremath{\mathbb{B}}}}\nolimits}
#+LaTeX_HEADER: \newcommand\ind[1]{\mathop{\mbox{\ensuremath{\mathbb{I}}}}\left\{#1\right\}}
#+LaTeX_HEADER: \renewcommand \Pr {\mathop{\mbox{\ensuremath{\mathbb{P}}}}\nolimits}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmax}{arg\,max}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmin}{arg\,min}
#+LaTeX_HEADER: \DeclareMathOperator*{\sgn}{sgn}
#+LaTeX_HEADER: \newcommand \defn {\mathrel{\triangleq}}
#+LaTeX_HEADER: \newcommand \Reals {\mathbb{R}}
#+LaTeX_HEADER: \newcommand \Param {\Theta}
#+LaTeX_HEADER: \newcommand \param {\theta}
#+LaTeX_HEADER: \newcommand \vparam {\vectorsym{\theta}}
#+LaTeX_HEADER: \newcommand \mparam {\matrixsym{\Theta}}
#+LaTeX_HEADER: \newcommand \bW {\matrixsym{W}}
#+LaTeX_HEADER: \newcommand \bw {\vectorsym{w}}
#+LaTeX_HEADER: \newcommand \bx {\vectorsym{x}}
#+LaTeX_HEADER: \newcommand \wi {\vectorsym{w}_i}
#+LaTeX_HEADER: \newcommand \wij {w_{i,j}}
#+LaTeX_HEADER: \newcommand \bA {\matrixsym{A}}
#+LaTeX_HEADER: \newcommand \ai {\vectorsym{a}_i}
#+LaTeX_HEADER: \newcommand \aij {a_{i,j}}
#+LaTeX_HEADER: \newcommand \bel {\beta}
#+LaTeX_HEADER: \newcommand \Ber {\textrm{Bernoulli}}
#+LaTeX_HEADER: \newcommand \Beta {\textrm{Beta}}
#+LaTeX_HEADER: \newcommand \Normal {\textrm{Normal}}
#+LaTeX_HEADER: \newcommand \Mult {\textrm{Mult}}
#+LaTeX_HEADER: \newcommand \Wish {\textrm{Wish}}
#+LaTeX_HEADER: \newcommand \Dir {\textrm{Dir}}
#+LaTeX_HEADER: \newcommand \Simplex {\mathbb{\Delta}}
#+LaTeX_HEADER: \usepackage[bbgreekl]{mathbbol}
#+LaTeX_HEADER: \tikzstyle{utility}=[diamond,draw=black,draw=blue!50,fill=blue!10,inner sep=0mm, minimum size=8mm]
#+LaTeX_HEADER: \tikzstyle{select}=[rectangle,draw=black,draw=blue!50,fill=blue!10,inner sep=0mm, minimum size=6mm]
#+LaTeX_HEADER: \tikzstyle{hidden}=[dashed,draw=black,fill=red!10]
#+LaTeX_HEADER: \tikzstyle{RV}=[circle,draw=black,draw=blue!50,fill=blue!10,inner sep=0mm, minimum size=6mm]
#+LaTeX_CLASS_OPTIONS: [smaller]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:3

* Latent variable models
** Introduction
*** Types of latent variables
- Mixture model
- Latent state in a dynamical system
- Personal information
- Objects in an image  
** Gaussian mixture models
*** Gaussian Mixture Models

In this model, data is generated from one of $k$ Gaussian
distributions with unknown mean and variance.

\begin{tikzpicture}
\node[RV] at (0,1) (x) {$x_t$};
\node[RV] at (1,1) (x2) {$x_{t+1}$};
\node[RV,hidden] at (0,0) (cat) {$\vparam$};
\node[RV,hidden] at (1,0) (mean) {$\vectorsym{\mu}$};
\node[RV,hidden] at (2,0) (var) {$\vectorsym{\Sigma}$};
\draw[->] (cat) to (x);
\draw[->] (mean) to (x);
\draw[->] (var) to (x);
\draw[->] (cat) to (x2);
\draw[->] (mean) to (x2);
\draw[->] (var) to (x2);
\end{tikzpicture}

**** Basic model
\begin{align}
c_t \mid \vparam &\sim \Mult(\vparam),\\
\bx_t \mid \vectorsym{\mu}_i, \matrixsym{\Sigma}_i, c_t = i & \sim \Normal(\vectorsym{\mu}_i, \matrixsym{\Sigma}_i).
\end{align}

**** Variables
- $c_t \in [k]$ specifies which distribution generated the t-th example.
- $\vparam \in \Simplex^k$ is the parameter of the distribution generating $c_t$
- $x_t \in \Reals^n$ is the t-th example
- $\vectorsym{\mu}_i \in \Reals^n$ - is the mean of the i-th Gaussian
- $\matrixsym{\Sigma}_i \succcurlyeq 0 \in \Reals^{n \times n}$ is the covariance of the i-th Gaussian
*** Maximum Likelihood Inference for Gaussian Mixture Models
**** Maximum likelihood
- Use the EM algorithm to solve
  \[
  \max_{\vparam, \vectorsym{\mu}, \vectorsym{\Sigma}} P(x_t \mid \vparam, \vectorsym{\mu}, \vectorsym{\Sigma})
  \]
  by adding the latent variable into the mix:
  \[
  \max_{\vparam, c_i, \vectorsym{\mu}, \vectorsym{\Sigma}} P(x_1, \ldots, x_t \mid c_1, \ldots, c_t, \vparam, \vectorsym{\mu}, \vectorsym{\Sigma})
  \]
*** Bayesian Inference for Gaussian Mixture Models
**** Graphical model
\begin{tikzpicture}
\node[RV] at (0,1) (x) {$x_t$};
\node[RV] at (1,1) (x2) {$x_{t+1}$};
\node[RV,hidden] at (0,0) (cat) {$\vparam$};
\node[RV,hidden] at (1,0) (mean) {$\vectorsym{\mu}$};
\node[RV,hidden] at (2,0) (var) {$\vectorsym{\Sigma}$};
\draw[->] (cat) to (x);
\draw[->] (mean) to (x);
\draw[->] (var) to (x);
\draw[->] (cat) to (x2);
\draw[->] (mean) to (x2);
\draw[->] (var) to (x2);
\node[RV] at (0,-1) (aprior) {$\vectorsym{\alpha}$};
\node[RV] at (1,-1) (gprior) {$\vectorsym{v}$};
\node[RV] at (2,-1) (sprior) {$\vectorsym{W}$};
\draw[->] (aprior) to (cat);
\draw[->] (gprior) to (mean);
\draw[->] (sprior) to (var);
\end{tikzpicture}
**** Bayesian model
\begin{align}
\vectorsym{\mu_i} \mid \vectorsym{v} &\sim \Normal(\vectorsym{v}, \vectorsym{I}),\\
\vectorsym{\Sigma_i} \mid \vectorsym{W} &\sim \Wish(\vectorsym{W})\\
\vparam \mid \vectorsym{\alpha} &\sim \Dir(\vectorsym{\alpha}),\\
c_t \mid \vparam &\sim \Mult(\vparam),\\
\bx_t \mid \vectorsym{\mu}, \matrixsym{\Sigma}, c_t = i & \sim \Normal(\vectorsym{\mu}_i, \matrixsym{\Sigma}_i).
\end{align}

** Hidden Markov models
*** Hidden Markov Model
**** Generic Hidden Markov Model
\begin{tikzpicture}
\node[RV] at (-1,1) (x0) {$x_{t-1}$};
\node[RV] at (0,1) (x1) {$x_t$};
\node[RV] at (1,1) (x2) {$x_{t+1}$};
\node[RV,hidden] at (-1,0) (s0) {$s_{t-1}$};
\node[RV,hidden] at (0,0) (s1) {$s_t$};
\node[RV,hidden] at (1,0) (s2) {$s_{t+1}$};
\draw[->] (s0) to (x0);
\draw[->] (s1) to (x1);
\draw[->] (s2) to (x2);
\draw[->] (s0) to (s1);
\draw[->] (s1) to (s2);
\end{tikzpicture}
- $s_t$: The state of the model. It has the \alert{Markov} property.
- $x_t$: The observtions of the model. They are \alert{not} Markov.
- When the model parameters are known, we can infer the hidden states. This is called \alert{filtering}.
**** Different types of HMMs
- Discrete $s_t, x_t$: Used in string prediction.
- Continuous $s_t, x_t$: The Kalman filter. Used in automatic control.
- Discrete $s_t$, continuous $x_t$. Used in speech recognition.
  

** Recommendation systems
*** Recommendation systems
**** The setting
- At time t, client $x_t$ appears
- We give a recommendation $a_t$.
- The customer chooses $y_t$
- We obtain a reward $r_t = \rho(a_t, y_t) \in \Reals$.
**** The two problems in recommendation
- How to model user preferences
- What to recommend

*** The modelling problem
**** The setting
- Clients $x_t$ 
- Items $y_t$
- Ratings $z_t$

* Hierarchical models

** Hierarchical models
*** Multiple hypotheses
- 
*** Populations
- 2

