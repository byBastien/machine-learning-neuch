#+TITLE: Linear Regression
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \usepackage{tikz}
#+LaTeX_HEADER: \usepackage{amsmath}
#+LaTeX_HEADER: \usepackage{amssymb}
#+LaTeX_HEADER: \usepackage{isomath}
#+LaTeX_HEADER: \newcommand \E {\mathop{\mbox{\ensuremath{\mathbb{E}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Var {\mathop{\mbox{\ensuremath{\mathbb{V}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Bias {\mathop{\mbox{\ensuremath{\mathbb{B}}}}\nolimits}
#+LaTeX_HEADER: \newcommand\ind[1]{\mathop{\mbox{\ensuremath{\mathbb{I}}}}\left\{#1\right\}}
#+LaTeX_HEADER: \renewcommand \Pr {\mathop{\mbox{\ensuremath{\mathbb{P}}}}\nolimits}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmax}{arg\,max}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmin}{arg\,min}
#+LaTeX_HEADER: \DeclareMathOperator*{\sgn}{sgn}
#+LaTeX_HEADER: \newcommand \defn {\mathrel{\triangleq}}
#+LaTeX_HEADER: \newcommand \Reals {\mathbb{R}}
#+LaTeX_HEADER: \newcommand \Param {B}
#+LaTeX_HEADER: \newcommand \param {\beta}
#+LaTeX_HEADER: \newcommand \vparam {\vectorsym{\beta}}
#+LaTeX_HEADER: \newcommand \mparam {\matrixsym{B}}
#+LaTeX_HEADER: \newcommand \bW {\matrixsym{W}}
#+LaTeX_HEADER: \newcommand \bw {\vectorsym{w}}
#+LaTeX_HEADER: \newcommand \wi {\vectorsym{w}_i}
#+LaTeX_HEADER: \newcommand \wij {w_{i,j}}
#+LaTeX_HEADER: \newcommand \bA {\matrixsym{A}}
#+LaTeX_HEADER: \newcommand \ai {\vectorsym{a}_i}
#+LaTeX_HEADER: \newcommand \aij {a_{i,j}}
#+LaTeX_HEADER: \newcommand \bx {\vectorsym{x}}
#+LaTeX_HEADER: \newcommand \by {\vectorsym{y}}
#+LaTeX_HEADER: \newcommand \bel {\beta}
#+LaTeX_HEADER: \newcommand \Ber {\textrm{Bernoulli}}
#+LaTeX_HEADER: \newcommand \Beta {\textrm{Beta}}
#+LaTeX_HEADER: \newcommand \Normal {\textrm{Normal}}
#+LaTeX_CLASS_OPTIONS: [smaller]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:3
* The Linear Model
** Simple linear regression
*** Simple linear regression
**** Input and output
- Data pairs $(x_t, y_t)$, $t = 1, \ldots, T$.
- Input $x_t \in \Reals^n$
- Output $y_t \in \Reals$.
**** Modelling the conditional expectation $\E[y_t | x_t]$
- Parameters $\param \in \Reals^n$
- Function $f_\param : \Reals^n \to \Reals$, defined as
\[
f_\param(x_t) = \param^\top x_{t} = \sum_{i=1}^n \param_i x_{t,i}
\]
*** Linear models
\begin{tikzpicture}[domain=-1:3]
   \draw[dotted, color=gray] (-1.1,-1.1) grid (5.1,4.1);
   \draw[->] (0,0) -- (4,0) node[right] {$x$};
   \draw[->] (0,0) -- (0,4) node[above] {$y$};
   \draw[thick, color=blue]   plot (\x, {0 + \x * 1})  node[right] {$\beta = (0, 1)$};
   \draw[thick, color=magenta]   plot (\x, {1 - \x * 1/2})  node[right] {$\beta = (1, - 1/2)$};
   \draw[thick, color=red]   plot (\x, {1 - \x * 0})  node[right] {$\beta = (1,  0)$};
\end{tikzpicture}
\[
f_\param(x) = \param^\top x = \sum_{i=1}^n \param_i x_{i}
\]

***  Two views of the problem
**** Learning as optimisation
- Each value $f_\param(x)$ is a *prediction* about the value of $y$
- We suffer a loss $\ell(y, f_\param(x))$ for every example $(x,y)$ that we see.
- We want to minimise the average loss over the data 
- Ideally, we want to minimise the expected loss, but the distribution $P$ is unknown.
**** Learning as inference
- The parameters $\param$ define a *probabilistic model* $P_\param(y | x)$ for every value of $y$.
- We want to find the parameters giving the highest probability on the observed data.
- Ideally, we want to find the true conditional distribution $P(y | x)$.

*** Learning as Optimisation 
Miniminise mean-squared error.
\[
\min_\param \sum_{t=1}^T [y_t - f_\param(x_t)]^2
\]
*** Learning as inference
Assume a Gaussian noise model:
\[
y_t = f(x_t) + \epsilon_t,  \qquad \epsilon_t \sim \Normal(0, \sigma)
\]
This leads to the conditional density
\[
p_\param(y_t| x_t) 
\propto
\exp(-[y_t - f_\param(x_t)]^2 / 2\sigma^2)
\]
Maximising the log-likelihood is equivalent to minimising mean-squared error:
\[
\argmax_\param \sum \ln p_\param(y_t| x_t) = \argmin_\param \sum_t |y_t - f_\param(x_t)|^2
\]
** Multiple linear regression
*** Simple linear regression
**** Input and output
- Data pairs $(x_t, y_t)$, $t = 1, \ldots, T$.
- Input $x_t \in \Reals^n$
- Output $y_t \in \Reals^m$.
**** Modelling the conditional expectation $\E[y_t | x_t]$
- Parameters $\param \in \Reals^{n \times m}$
- Function $f_\param : \Reals^n \to \Reals^m$, defined as
\[
f_\param(x_t) = \param^\top x_{t} = \sum_{i=1}^n \param_i x_{t,i}
\]
*** Linear models
\begin{tikzpicture}[domain=-1:3]
   \draw[dotted, color=gray] (-1.1,-1.1) grid (5.1,4.1);
   \draw[->] (0,0) -- (4,0) node[right] {$x$};
   \draw[->] (0,0) -- (0,4) node[above] {$y$};
   \draw[thick, color=blue]   plot (\x, {0 + \x * 1})  node[right] {$\beta = (0, 1)$};
   \draw[thick, color=magenta]   plot (\x, {1 - \x * 1/2})  node[right] {$\beta = (1, - 1/2)$};
   \draw[thick, color=red]   plot (\x, {1 - \x * 0})  node[right] {$\beta = (1,  0)$};
\end{tikzpicture}
\[
f_\param(x) = \param^\top x = \sum_{i=1}^n \param_i x_{i}
\]

* Regression libraries in Python
** sklearn
*** sklearn
#+BEGIN_SRC python
**** Fitting a model to data
from sklearn.linear_model import LinearRegression
model = LinearRegression().fit(X, Y) # use X as is
**** Getting predicitons
Z = model.predict(X)


#+END_SRC python
** statsmodels
*** Statsmodels
**** Fitting a model to data X, Y
#+BEGIN_SRC python
  import statsmodels.api as sm
  Xa = sm.add_constant(X) # adds a constant factor to the data
  model = sm.OLS(Y, Xa)
  results = model.fit() 
#+END_SRC
**** Getting predictions
The prediction is not just a point!
#+BEGIN_SRC python
  z = results.get_prediction(Xa[t])
  z.predicted_mean # This is E[y|x]
#+END_SRC


* Optimisation algorithms
** Gradient Descent
*** Gradient descent algorithm
**** Minimising a function
\[
\min_\param f(\param) \geq f(\param') \forall \param',
\qquad \param^* = \argmin_\param f(\param) \Rightarrow f(\param^*) = \min_\param f(\param)
\]
**** Gradient descent for minimisation
- Input $\param_0$
- For $n = 0, \ldots, N$:
- $\param_{n+1} = \param_n - \eta_n \nabla_\param f(\param_n)$
**** Step-size $\eta_n$
- $\eta_n$ fixed: for online learning
- $\eta_n = c/[c + n]$ for asymptotic convergence
- $\eta_n = \argmin_\eta f(\theta_n + \eta \nabla_\param)$: Line search.

*** Gradient desecnt for squared error
**** Cost gradient
Using the chain rule of differentiation:
\begin{align*}
\nabla_\param \ell(\param)
&= \nabla \sum_{t=1}^T [y_t - \pi_\param(x_t)]^2
\\
&= \sum_{t=1}^T \nabla [y_t - \pi_\param(x_t)]^2
\\
&= \sum_{t=1}^T 2 [y_t - \pi_\param(x_t)] [- \nabla \pi_\param(x_t)]^2
\end{align*}
**** Parameter gradient
For a linear regressor:
\[
\frac{\partial}{\partial \param_j} \pi_\param(x_t) = x_{t,j}.
\]

*** Stochastic gradient descent algorithm
**** Note
 :PROPERTIES:
 :BEAMER_ENV: note
 :END:
For the general case, we got to do this.

**** When $f$ is an expectation
\[
f(\param) = \int_X dP(x) g(x, \param).
\]
**** Replacing the expectation with a sample:
\begin{align*}
\nabla f(\param)
&= \int_X dP(x) \nabla g(x, \param)\\
&\approx \frac{1}{K} \sum_{k=1}^K \nabla g(x^{(k)}, \param), && x^{(k)} \sim P.
\end{align*}

** Least-Squares
*** Some matrix algebra
**** The identity matrix $I \in \Reals^{n \times n}$
- For this matrix, $I_{i,i} = 1$ and $I_{i,j} = 0$ when $j \neq i$.
- $Ix = x$ and $IA = A$.

**** The inverse of a matrix $A \in \Reals^{n \times n}$
$A^{-1}$ is called the inverse of $A$ if
- $A A^{-1} = I$.
- or equivalently $A^{-1} A = I$.

**** The pseudo-inverse of a matrix $A \in \Reals^{n \times m}$
- $\tilde{A}^{-1}$ is called the *left pseudoinverse* of $A$ if $\tilde{A}^{-1} A = I$.
\[
\tilde{A}^{-1} = (A^\top A)^{-1} A^\top, \qquad n > m
\]
- $\tilde{A}^{-1}$ is called the *right pseudoinverse* of $A$ if $A \tilde{A}^{-1} = I$.
\[
\tilde{A}^{-1} =  A^\top (AA^\top)^{-1}, \qquad m > n
\]

*** Analytical Least-Squares Solution
We need to solve the following equations for $A$:
\begin{equation*}
\begin{matrix}
y_1 &= x_1^\top \param\\
\cdots & \cdots\\
y_t &= x_t^\top \param\\
\cdots & \cdots\\
y_T &= x_T^\top \param
\end{matrix}
\end{equation*}
We can rewrite it in matrix form:
\begin{equation*}
\begin{pmatrix}
y_1\\
\vdots\\
y_t\\
\vdots\\
y_T
\end{pmatrix}
= 
\begin{pmatrix}
x_1^\top\\
\vdots\\
x_t^\top\\
\vdots\\
x_T^\top
\end{pmatrix}
\param
\end{equation*}
Resulting in 
\[
\by = X \param
\]
So we can use the left-pseudo inverse $\tilde{X}^{-1}$ to obtain
\[
\param = \tilde{X}^{-1} \by
\]

* Interpretation of the fit
** Problem parameters
*** The coefficients
- $\param_i$ tells us how much $y$ is correlated with $x_{t,i}$
- However, multiple correlations might be evident.
** Exercises
*** Linear regression exercises
- Exercises 8, 13 from ISLP
- A variant of Ex. 13 but with Y generated independently of X.



