#+TITLE: Linear Regression
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \usepackage{tikz}
#+LaTeX_HEADER: \usepackage{amsmath}
#+LaTeX_HEADER: \usepackage{amssymb}
#+LaTeX_HEADER: \usepackage{isomath}
#+LaTeX_HEADER: \newcommand \E {\mathop{\mbox{\ensuremath{\mathbb{E}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Var {\mathop{\mbox{\ensuremath{\mathbb{V}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Bias {\mathop{\mbox{\ensuremath{\mathbb{B}}}}\nolimits}
#+LaTeX_HEADER: \newcommand\ind[1]{\mathop{\mbox{\ensuremath{\mathbb{I}}}}\left\{#1\right\}}
#+LaTeX_HEADER: \renewcommand \Pr {\mathop{\mbox{\ensuremath{\mathbb{P}}}}\nolimits}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmax}{arg\,max}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmin}{arg\,min}
#+LaTeX_HEADER: \DeclareMathOperator*{\sgn}{sgn}
#+LaTeX_HEADER: \newcommand \defn {\mathrel{\triangleq}}
#+LaTeX_HEADER: \newcommand \Reals {\mathbb{R}}
#+LaTeX_HEADER: \newcommand \Param {B}
#+LaTeX_HEADER: \newcommand \param {\beta}
#+LaTeX_HEADER: \newcommand \vparam {\vectorsym{\beta}}
#+LaTeX_HEADER: \newcommand \mparam {\matrixsym{B}}
#+LaTeX_HEADER: \newcommand \bW {\matrixsym{W}}
#+LaTeX_HEADER: \newcommand \bw {\vectorsym{w}}
#+LaTeX_HEADER: \newcommand \wi {\vectorsym{w}_i}
#+LaTeX_HEADER: \newcommand \wij {w_{i,j}}
#+LaTeX_HEADER: \newcommand \bA {\matrixsym{A}}
#+LaTeX_HEADER: \newcommand \ai {\vectorsym{a}_i}
#+LaTeX_HEADER: \newcommand \aij {a_{i,j}}
#+LaTeX_HEADER: \newcommand \bx {\vectorsym{x}}
#+LaTeX_HEADER: \newcommand \by {\vectorsym{y}}
#+LaTeX_HEADER: \newcommand \bel {\beta}
#+LaTeX_HEADER: \newcommand \Ber {\textrm{Bernoulli}}
#+LaTeX_HEADER: \newcommand \Beta {\textrm{Beta}}
#+LaTeX_HEADER: \newcommand \Normal {\textrm{Normal}}
#+LaTeX_CLASS_OPTIONS: [smaller]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:3
* Regression
** Regression
*** Regression example

*** Simple regression
**** Input and output
- Data pairs $(\bx_t, y_t)$, $t = 1, \ldots, T$.
- Input $\bx_t \in \Reals^n$
- Output $y_t \in \Reals$.
**** What is a good prediction
- At time step $t > T$
- Observe $x_t$.
- Make a prediction $a_t$.
- Observe the actual $y_t$
- Suffer a loss $\ell(a_t, y_t) = r(a_t, y_t)$.
**** Goal
- Find a policy to make predictions to minimise the loss!

* The Linear Model
** Single-variable models
*** Single-variable regression
- $x$: height
- $y$: weight
\[
y \approx \param_0 + \param_1 x
\]

** Test
*** Simple linear regression
**** Input and output
- Data pairs $(\bx_t, y_t)$, $t = 1, \ldots, T$.
- Input $x_t \in \Reals^n$
- Output $y_t \in \Reals$.
**** Predicting the conditional mean $\E[y_t | \bx_t]$
- Parameters $\vparam \in \Reals^n$
- Function $f_\vparam : \Reals^n \to \Reals$, defined as
\[
f_\vparam(\bx_t) = \vparam^\top \bx_{t} = \sum_{i=1}^n \vparam_i x_{t,i}
\]

*** Python implementation
**** statsmodels
#+BEGIN_SRC python
import statsmodels.api as sm
Xa = sm.add_constant(X) # we need to add a bias term here
model = sm.OLS(Y, Xa)
results = model.fit() 
prediction = results.get_prediction(Xa[0]) # get prediction
prediction.predicted_mean # this is a point predictio
#+END_SRC

**** scikitlearn 
#+BEGIN_SRC python
from sklearn.linear_model import LinearRegression
X = X.reshape(-1,1)  # if there is only one feature, you must do this ...
Y = Y.reshape(-1,1)  # if there is only one target, you must do this ...
model = LinearRegression().fit(X, Y) 
Z = model.predict(X) # you can get all the predictions

#+END_SRC



***  Two views of the problem

**** Learning as Optimisation 
Miniminise mean-squared error.
\[
\min_\vparam \sum_{t=1}^T [y_t - f_\vparam(\bx_t)]^2
\]
**** Learning as inference
Assume a Gaussian noise model:
\[
y_t = f(\bx_t) + \epsilon_t,  \qquad \epsilon_t \sim \Normal(0, \sigma)
\]
This leads to the conditional density
\[
p_\vparam(y_t| \bx_t) 
\propto
\exp(-[y_t - f_\vparam(\bx_t)]^2 / 2\sigma^2)
\]
Maximising the log-likelihood is equivalent to minimising mean-squared error:
\[
\argmax_\vparam \sum \ln p_\vparam(y_t| \bx_t) = \argmin_\vparam \sum_t |y_t - f_\vparam(\bx_t)|^2
\]
* Optimisation algorithms
** Gradient Descent
*** Gradient descent algorithm
**** Minimising a function
\[
\min_\vparam f(\vparam) \geq f(\vparam') \forall \vparam',
\qquad \vparam^* = \argmin_\vparam f(\vparam) \Rightarrow f(\vparam^*) = \min_\vparam f(\vparam)
\]
**** Gradient descent for minimisation
- Input $\vparam_0$
- For $n = 0, \ldots, N$:
- $\vparam_{n+1} = \vparam_n - \eta_n \nabla_\vparam f(\vparam_n)$
**** Step-size $\eta_n$
- $\eta_n$ fixed: for online learning
- $\eta_n = c/[c + n]$ for asymptotic convergence
- $\eta_n = \argmin_\eta f(\theta_n + \eta \nabla_\vparam)$: Line search.

*** Gradient desecnt for squared error
**** Cost gradient
Using the chain rule of differentiation:
\begin{align*}
\nabla_\vparam \ell(\vparam)
&= \nabla \sum_{t=1}^T [y_t - \pi_\vparam(\bx_t)]^2
\\
&= \sum_{t=1}^T \nabla [y_t - \pi_\vparam(\bx_t)]^2
\\
&= \sum_{t=1}^T 2 [y_t - \pi_\vparam(\bx_t)] [- \nabla \pi_\vparam(\bx_t)]^2
\end{align*}
**** Parameter gradient
For a linear regressor:
\[
\frac{\partial}{\partial \vparam_j} \pi_\vparam(\bx_t) = x_{t,j}.
\]

*** Stochastic gradient descent algorithm
**** Note
 :PROPERTIES:
 :BEAMER_ENV: note
 :END:
For the general case, we got to do this.

**** When $f$ is an expectation
\[
f(\vparam) = \int_X dP(x) g(x, \vparam).
\]
**** Replacing the expectation with a sample:
\begin{align*}
\nabla f(\vparam)
&= \int_X dP(x) \nabla g(x, \vparam)\\
&\approx \frac{1}{K} \sum_{k=1}^K \nabla g(x^{(k)}, \vparam), && x^{(k)} \sim P.
\end{align*}

** Least-Squares
*** Some matrix algebra
**** The identity matrix $I \in \Reals^{n \times n}$
- For this matrix, $I_{i,i} = 1$ and $I_{i,j} = 0$ when $j \neq i$.
- $Ix = x$ and $IA = A$.

**** The inverse of a matrix $A \in \Reals^{n \times n}$
$A^{-1}$ is called the inverse of $A$ if
- $A A^{-1} = I$.
- or equivalently $A^{-1} A = I$.

**** The pseudo-inverse of a matrix $A \in \Reals^{n \times m}$
- $\tilde{A}^{-1}$ is called the *left pseudoinverse* of $A$ if $\tilde{A}^{-1} A = I$.
\[
\tilde{A}^{-1} = (A^\top A)^{-1} A^\top, \qquad n > m
\]
- $\tilde{A}^{-1}$ is called the *right pseudoinverse* of $A$ if $A \tilde{A}^{-1} = I$.
\[
\tilde{A}^{-1} =  A^\top (AA^\top)^{-1}, \qquad m > n
\]

*** Analytical Least-Squares Solution
We need to solve the following equations for $A$:
\begin{equation*}
\begin{matrix}
y_1 &= \bx_1^\top \vparam\\
\cdots & \cdots\\
y_t &= \bx_t^\top \vparam\\
\cdots & \cdots\\
y_T &= \bx_T^\top \vparam
\end{matrix}
\end{equation*}
We can rewrite it in matrix form:
\begin{equation*}
\begin{pmatrix}
y_1\\
\vdots\\
y_t\\
\vdots\\
y_T
\end{pmatrix}
= 
\begin{pmatrix}
\bx_1^\top\\
\vdots\\
\bx_t^\top\\
\vdots\\
\bx_T^\top
\end{pmatrix}
\vparam
\end{equation*}
Resulting in 
\[
\by = X \vparam
\]
So we can use the left-pseudo inverse $\tilde{X}^{-1}$ to obtain
\[
\vparam = \tilde{X}^{-1} \by
\]

* Interpretation and practical use
** Problem parameters
*** The coefficients
- $\param_i$ tells us how much $y$ is correlated with $x_{t,i}$
- However, multiple correlations might be evident.
** When the assumptions to do not fit the model
*** Nonlinear relations
- Maybe the actual function linking $x,y$ is not linear:
  \[
  \]
*** Correlation of error terms
*** Non-constant variance
*** Outliers
*** High leverage points
*** Colinearity

** Exercises
*** Linear regression exercises
- Exercises 8, 13 from ISLP
- A variant of Ex. 13 but with Y generated independently of X.



