#+TITLE: Nearest Neighbour Algorithms
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \include{preamble}
#+LaTeX_CLASS_OPTIONS: [10pt]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:3
#+latex_header: \AtBeginSection[]{\begin{frame}<beamer>\tableofcontents[currentsection]\end{frame}}
* Introduction
** The hidden secret of machine learning
*** Supervised learning

- Given labelled training examples $(x_1, y_1), \ldots (x_T, y_T)$ where
- $x_t \in X$ are *features* 
- $y_t \in Y$ are *labels*..
**** Feature space $\CX$
- Usually $\CX = \Reals^n$: the n-dimensional Euclidean space
- How do we use your class data?
**** Classification
- $Y = \{1, \ldots, m\}$ are *discrete* labels
**** Regression
- $Y = \Reals^m$ are *continuous* values

*** The kNN algorithm idea

- Assume an unknown example is similar to its neighbours
- Smoothness allows us to make predictions

/Discriminatory analysis-nonparametric discrimination: consistency properties/, Evelyn Fix and Joseph L.  Hodges Jr, 1951.

**** Evelyn :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
#+CAPTION: Evelyn Fix
#+ATTR_LATEX: :width 0.5\textwidth
[[../fig/fix_evelyn2.jpg]]
**** Joseph :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
#+CAPTION: Joseph Hodges
#+ATTR_LATEX: :width 0.5\textwidth
[[../fig/Hodges.jpg]]





*** Performance of KNN on image classification
[[../fig/knn-image-performance.png]]

- Really simple!
- Can outperform really complex models!

* The algorithm
** $k$ Nearest Neighbours

*** The Nearest Neighbour algorithm
**** Pseudocode
- Input: Data $(x_t, y_t)_{t=1}^T$, test point $x$, distance $d$ 
- $t^* = \argmin_t d(x_t, x)$ /// How do we implement this?
- Return $\hat{y}_t = y_{t^*}$

**** Classification
     $\hat{y}_t  \in [m] \equiv \{1, \ldots, m\}$
     
**** Regression
$\hat{y}_t  \in \Reals^m$

*** The k-Nearest Neighbour algorithm

**** Pseudocode
- Input: Data $(x_t, y_t)_{t=1}^T$, test point $x$, distance $d$, neighbours \(k\)
- Calculate $h_t = d(x_t, x)$ for all $t$.
- Get sorted indices $s = \texttt{argsort}(h)$ so that $d(x_{s_i}, x) \leq d(x_{s_{i+1}}, x)$ for all $i$. 
- Return $\sum_{i=1}^k y_{s_i} / k$.
#+BEAMER: \pause
**** Classification
- We use a *one-hot encoding* $(0, \ldots, 0, 1, 0, \ldots, 0)$, with $y_t \in \{0,1\}^m$.
- The class of the \(t\)-th example is $j$ $\Leftrightarrow$ $y_{t,j} = 1$.
- Equivalently, return $p$ with 
\[
p_i = \sum_{t=1}^k (y_{s_t} = i)
\]
#+BEAMER: \pause
**** Regression
- $y_t  \in \Reals^m$, so we need do nothing

*** Making a decision
**** kNN Output
- Given features $x$, we get a vector
- $p_i = \hat{\Pr}(y = i | x)$.
**** Accuracy
- Predicted label $a$
- Actual label $y$
- $U(a, y) = \ind{a = y}$
**** Classification decision to maximise accuracy
- $a_t = \argmax_i p_i$

*** Making a decision: spam
**** kNN Output
- Given features $x$, we get a vector
- $p_i = \hat{\Pr}(y = i | x)$.
**** Minimise spam annoyance
What utility function would you use for the spam detection problem
|----------+------+------+-------|
| Utility  | Pass | Flag | Trash |
|----------+------+------+-------|
| Normal   |      |      |       |
| Spam     |      |      |       |
| Phishing |      |      |       |
| Virus    |      |      |       |
|----------+------+------+-------|

**** Classification decision to maximise accuracy
- $a_t = \argmax_i p_i$

  
** Extensions and parameters
*** The number of neighbours
**** $k=1$
- How does it perform on the training data?
- How might it perform on unseen data?
**** $k = T$
- How does it perform on the training data?
- How might it perform on unseen data?

*** Distance function
**** For data in $\Reals^n$, \(p\)-norm
\[
d(x,y) = \|x - y\|_p
\]
**** Scaled norms
When features having varying scales:
\[
d(x,y) = \|S x - S y\|_p
\]
Or pre-scale the data

**** Complex data
- Manifold distances
- Graph distance

*** Distances 
**** A distance $d(\cdot, \cdot)$:
- Identity $d(x,x) = 0$.
- Positivity $d(x,y) > 0$ if $x \neq y$.
- Symmetry $d(y,x) = d(x,y)$.
- Triangle inequality $d(x,y) \leq d(x,z) + d(z,y)$.
**** For data in $\Reals^n$, $p$-norm
\[
d(x,y) = \|x - y\|_p
\]
*** Norms;
**** A norm $\|\cdot\|$
- Zero element $\|0\| = 0$.
- Homogeneity $\|cx\| = c \|x\|$ for any scalar $a$.
- Triangle inequality $\|x + y\| \leq \|x\| + \|y\|$.
**** $p$-norm
\[
\|z\|_p = \left(\sum_i z_i^p\right)^{1/p}
\]
*** Neighbourhood calculation
If we have $T$ datapoints
**** Sort and top $K$.
- Requires $O(T \ln T)$ time
**** Use the Cover-Tree or KD-Tree algorithm
- Requires $O(c K \ln T)$ time.
- $c$ depends on the data distribution.

  
* Activities
*** KNN activity
- Implement nearest neighbours
- Introduction to scikitlearn nearest neighbours

