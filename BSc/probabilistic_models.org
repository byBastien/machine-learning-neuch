#+TITLE: Probabilistic Models
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \include{preamble}
#+LaTeX_CLASS_OPTIONS: [smaller]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:3
#+latex_header: \AtBeginSection[]{\begin{frame}<beamer>\frametitle{Topic}\tableofcontents[currentsection]\end{frame}}

* The principle of probabilistic Modelling
** Probabilistic Models
*** Probabilistic modelling
**** The problem
- Model family $\{P_\param : \param \in \Param\}$
- Each model assigns a probability $P_\param(x)$ to the data $x$.
- How can we estimate $\param$ from $x$?
**** Maximum Likelihood (ML) Estimation
$\hat{\theta}(x) = \argmax_\theta P_\param(x)$.

**** Maximum A Posteriori (MAP) Estimation
Here we also need a prior distribution, but still estimate a single parameter:
- Prior $\bel(\param)$, a distribution on $\Param$.
- $\hat{\param}(x) = \argmax_\param P_\param(x) \bel(\param)$.
**** Bayesian Estimation
Here we estimate the complete distribution over parameters
- $\bel(\param | x) = P_\param(x) \bel(\param) / \sum_{\param'} P_{\param'}(x) \bel(\param')$ 
*** Maximum Likelihood
*** Maximum A Posterior
*** Bayesian Estimation
* Examples
** The Bernoulli Distribution
*** The Bernoulli distribution: Modelling a coin
**** Definition
If $x_t \sim \Ber(\param)$ then
$x_t = 1$ w.p. $\param$ and $x_t = 0$ w.p. $1 - \param$.
**** Likelihood function
 \(P(x_1, \ldots, x_T | \param) = \prod_{t=1}^T P(x_t | \param)\) = \prod_{t=1}^T \theta^{x_t} (1 - \theta)^{1 - x_t}
**** Maximum Likelihood Estimate
$\argmax_\param P(x | \param) = \argmax_\param \ln P(x | \param)$.
\begin{align*}
\frac{d}{d\param} \ln P(x | \param)
&=  \frac{d}{d\param} [\sum_t \ln P(x_t | \param)]
= \frac{d}{d\param} [\sum_t \ln \theta^{x_t} (1 - \theta)^{1 - x_t}]
\\
&=
\frac{d}{d\param}[ \ln (\theta) \sum_t x_t +  \ln (1 - \theta) \sum_t (1 - x_t)]
\\
&=
\frac{1}{\theta} \sum_t x_t  - \frac{1}{1 - \theta} \sum_t (1 - x_t)
\end{align*}
Setting the derivative to zero: \[\hat{\param}_T = \frac{1}{T} \sum_{t=1}^T x_t\]

*** Bayesian Estimate
**** The prior distribution $P(\param)$
$\param \sim \Beta(\alpha_1, \alpha_0)$
**** The likelihood function $P(x | \param)$
 \(P(x_1, \ldots, x_T | \param) = \prod_{t=1}^T P(x_t | \param)\)
**** The posterior distribution $P(\param | x)$
$\param \sim \Beta(\alpha_1 + \sum_{t=1}^T x_t, \alpha_0 + \sum_{t=1}^T x_t)$.

** The Gaussian distribution
*** The Gaussian distribution 
**** Definition
If $x_t \sim \Normal(\mu, \sigma)$ then it has density
\[
p(x) = \frac{1}{\sqrt{2\pi} \sigma} \exp(-2|x - \mu|^2/\sigma^2)
\]
**** Maximum Likelihood Estimate
Select
\[
\hat{\mu}(D) = \frac{1}{T} \sum_{t=1}^T x_t,
\qquad
\hat{\sigma}(D) = \frac{1}{T} \sum_{t=1}^T [x_t - \hat{\mu}(D)]^2
\]

