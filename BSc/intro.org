#+TITLE: Introduction to Machine Learning
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \usepackage{tikz}
#+LaTeX_HEADER: \usepackage{amsmath}
#+LaTeX_HEADER: \usepackage{amssymb}
#+LaTeX_HEADER: \usepackage{isomath}
#+LaTeX_HEADER: \newcommand \E {\mathop{\mbox{\ensuremath{\mathbb{E}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Var {\mathop{\mbox{\ensuremath{\mathbb{V}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Bias {\mathop{\mbox{\ensuremath{\mathbb{B}}}}\nolimits}
#+LaTeX_HEADER: \newcommand\ind[1]{\mathop{\mbox{\ensuremath{\mathbb{I}}}}\left\{#1\right\}}
#+LaTeX_HEADER: \renewcommand \Pr {\mathop{\mbox{\ensuremath{\mathbb{P}}}}\nolimits}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmax}{arg\,max}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmin}{arg\,min}
#+LaTeX_HEADER: \DeclareMathOperator*{\sgn}{sgn}
#+LaTeX_HEADER: \newcommand \defn {\mathrel{\triangleq}}
#+LaTeX_HEADER: \newcommand \Reals {\mathbb{R}}
#+LaTeX_HEADER: \newcommand \Param {\Theta}
#+LaTeX_HEADER: \newcommand \param {\theta}
#+LaTeX_HEADER: \newcommand \vparam {\vectorsym{\theta}}
#+LaTeX_HEADER: \newcommand \mparam {\matrixsym{\Theta}}
#+LaTeX_HEADER: \newcommand \bW {\matrixsym{W}}
#+LaTeX_HEADER: \newcommand \bw {\vectorsym{w}}
#+LaTeX_HEADER: \newcommand \wi {\vectorsym{w}_i}
#+LaTeX_HEADER: \newcommand \wij {w_{i,j}}
#+LaTeX_HEADER: \newcommand \bA {\matrixsym{A}}
#+LaTeX_HEADER: \newcommand \ai {\vectorsym{a}_i}
#+LaTeX_HEADER: \newcommand \aij {a_{i,j}}
#+LaTeX_HEADER: \newcommand \bx {\vectorsym{x}}
#+LaTeX_HEADER: \newcommand \bX {\matrixsym{X}}
#+LaTeX_HEADER: \newcommand \bel {\beta}
#+LaTeX_HEADER: \newcommand \Ber {\textrm{Bernoulli}}
#+LaTeX_HEADER: \newcommand \Beta {\textrm{Beta}}
#+LaTeX_HEADER: \newcommand \Normal {\textrm{Normal}}
#+LaTeX_CLASS_OPTIONS: [smaller]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:3

* The problems of Machine Learning (1 week)
  #+TOC: headlines [currentsection,hideothersubsections]
** Introduction
*** Machine Learning And Data Mining
**** The nuts and bolts
- Models
- Algorithms
- Theory
- Practice
**** Workflow
- Scientific question
- Formalisation of the problem
- Data collection
- Analysis and model selection
**** Types of machine learning / statistics problems
- Classification
- Regression
- Density estimation
- Clustering
- Reinforcement learning

*** Machine learning
**** Data Collection
- Downloading a clean dataset from a repository
- Performing a survey
- Scraping data from the web
- Deploying sensors, performing experiments, and obtaining measurements.
**** Modelling (what we focus on this course)
- Simple: the bias of a coin
- Complex:  a language model.
- The model depends on the data and the problem
**** Algorithms and Decision Making
- We want to use models to make decisions.
- Decisions are made every step of the way.
- Decisions are automated algorithmically.
  
*** The main problems in machine learning and statistics
- Inference (what is going on now? what happened in the past?)
- Prediction (of the future, from the past)
- Decision making (what should we do to achieve our goals?)

Sometimes each problem can be solved in isolation, but we usually have to partially solve all problems at the same time.
*** Prediction
[[./fig/meteo.png]]
- Will it rain tomorrow?
- How much will bitcoin be worth next year?
- When is the next solar eclipse?

*** Inference
[[./fig/gravity.jpg]]
- Does my poker opponent have two aces?
- What is the law of gravitation?

*** Decision Making
[[./fig/lunar.png]]
[[./fig/artemis.gif]]

- Should I go hiking tomorrow?
- Should I buy some bitcoins?
- Should I fold, call, or raise in my poker game?
- How can I get a spaceship to the moon and back?

*** The need to learn from data
**** Problem definition
- What problem do we need to solve?
- How can we formalise it?
- What properties of the problem can we learn from data?

**** Data collection
- Why do we need data?
- What data do we need?
- How much data do we want?
- How will we collect the data?

**** Modelling and decision making
- How will we compute something useful?
- How can we use the model to make decisions?


* Estimation
** Answering a scientific problem
*** Problem definition
- Health, weight and height
****  Health questions regarding height and weight
- What is a normal height and weight?
- How are they related to health?
- What variables affect height and weight?

*** Data collection
Think about which variables we need to collect to answer our research question.

**** Necessary variables
The variables we need to know about
- Weight
- Height
- Health issue:
**** Auxiliary variables
Factors related to height, weight a aitnd health

**** Possible confounders
Other factors that might affect health outcomes, unrelated to height and weight

*** Class data                                                     :activity:
- The class enters their data into the [[https://docs.google.com/spreadsheets/d/1xRpo1LuMz62Yu57ABxtkvbvCebuew3VUh387ElXNoGU/edit?usp=sharing][excel file]]. 
**** Mean estimation
- What is the average height/disease prevalence?
- What is the expected height/disease prevalence in the general population?

**** Supervised learning problems:
- Classification: Can we predict gender from height/weight?
- Regression: Can we predict weight from height and gender?
- In both cases we predict *output* variables from *input* variables
**** *Input* variables
Also called features, predictors, independent variables

**** *Output* variables
Also called response, or dependent variables.




*** Variables
The class data looks like this

|------------+--------+--------+--------+-----+-------------+---------|
| First Name | Gender | Height | Weight | Age | Nationality | Smoking |
|------------+--------+--------+--------+-----+-------------+---------|
| Lee        | M      |    170 |     80 |  20 | Chinese     |      10 |
| Fatemeh    | F      |    150 |     65 |  25 | Turkey      |       0 |
| Ali        | Male   |    174 |     82 |  19 | Turkish     |       0 |
| Joan       | N      |   5'11 |    180 |  21 | Brtish      |       4 |
|------------+--------+--------+--------+-----+-------------+---------|

- $\bX$: Everybody's data
- $x_t$: The t-th person's data
- $x_{t,k}$: The k-th feature of the $t$-th person.
- $\bx_k$: Everybody's k-th feature

**** Raw versus neat data
- Neat data: $x_t \in \Reals^n$
- Raw data: text, graphs, missing values, etc

*** Python pandas for data wrangling
**** Reading class data
#+BEGIN_SRC python
import pandas as pd
X = pd.read_excel("data/class.xlsx")
X["First Name"]
#+END_SRC

#+RESULTS:
: None

- Array columns correspond to features
- Columns can be accessed through namesx

**** Summarising class data
#+BEGIN_SRC python :exports code
X.hist()
import matplotlib.pyplot as plt
plt.show()
#+END_SRC

#+RESULTS:

*** Pandas and DataFrames
- Data in pandas is stored in a *DataFrame*
- DataFrame is *not the same* as a numpy array.
**** Core libraries
#+BEGIN_SRC python :exports code
import pandas as pd
import numpy as np
#+END_SRC

**** Series: A sequence of values
     :PROPERTIES:
     :BEAMER_opt:   [shrink=15]
     :END:
#+BEGIN_SRC python :exports code
# From numpy array:
s = pd.Series(np.random.randn(3),  index=["a", "b", "c"])
# From dict:
d = {"a": 1, "b": 0, "c": 2}
s = pd.Series(d)
# accessing elemets
s.iloc[2] #element 2
s.iloc[1:2] #elements 1,2
s.array # gets the array object 
s.to_numpy() # gets the underlying numpy array
#+END_SRC

*** DataFrames


**** Constructing from a numpy array
#+BEGIN_SRC python :exports code
data = np.random.uniform(size = [3,2])
df = pd.DataFrame(data, index=["John", "Ali", "Sumi"],
         columns=["X1", "X2"])
#+END_SRC

**** Constructing from a dictionary
#+BEGIN_SRC python :exports code
d = {  "one": pd.Series([1, 2], index=["a", "b"]),
       "two": pd.Series([1, 2, 3], index=["a", "b", "c"])}
df = pd.DataFrame(d)
#+END_SRC



**** Access
#+BEGIN_SRC python :exports code
X["First Name"] # get a column
X.loc[2] # get a row
X.at[2, "First Name"] # row 2, column 'first name'
X.loc[2].at["First Name"] # row 2, element 'first name' of the series
X.iat[2,0] # row 2, column 0
#+END_SRC



** Simple modelling
*** Means using python
**** Calculating the mean of a random variable
#+BEGIN_SRC python
import numpy as np
X = np.random.gamma(170, 1, size=20)
X.mean()
np.mean(X)
#+END_SRC

**** Calculating the mean of our class data
#+BEGIN_SRC python
X.mean() # gives the mean of all the variables through pandas.core.frame.DataFrame
X["Height"].mean()
np.mean(X["Weight"])
#+END_SRC


*** One variable: expectations and distributions
**** Modelling the height 
- Mean: models the expected value
- Variance: models the \ldots variance
- Empirical distribution: models the distribution
**** The expected value and the mean
Assume $x_t : \Omega \to \Reals$, and $\omega \sim P$
- $x_1, \ldots, x_t, \ldots, x_T$: random i.i.d. variables
- $\Omega$: random outcome space
- $P$: distribution of outcomes $\omega \in \Omega$
- $\E_p[x]$: expectation of $x$ under $P$
\[
\E_P[x_t] 
= \sum_{\omega \in \Omega}  x_t(\omega) P(\omega) 
\approx
\frac{1}{T} \sum_{t=1}^T x_{t}
\]
- The sample mean is \(O(1/\sqrt{T})\)-close to the expected value.

*** Reminder: expectations of random variables
**** A gambling game
What are the expected winnings if you play this game?
- [a] With probability 1%, you win 100 CHF
- [b] With probability 40%, you win 20 CHF.
- [c] Otherwise, you win nothing
**** Solution
#+BEAMER: \pause
- Let $x$ be the amount won, then $x(a) = 100, x(b) = 20, x(c) = 0$.
- We need to calculate
\[
\E_P(x) = \sum_{\omega \in \{a, b, c\}} \!\!\! x(\omega) P(\omega) =
x(a) P(a) + x(b) P(b) + x(c) P(c) 
\]
- $P(c) = 59\%$, as $P(\Omega) = 1$. Substituting,
\[
\E_P(x) = 1 + 8 + 0 = 9.
\]
*** Populations, samples, and distributions
**** The world
    :PROPERTIES:
    :BEAMER_col: 0.6
    :END:
#+CAPTION: The world population
#+NAME:   fig:world
[[./fig/population.png]]
**** A sample
    :PROPERTIES:
    :BEAMER_col: 0.4
    :END:
#+CAPTION: A sample
#+NAME:   fig:sample
[[./fig/sample.png]]
*** Statistical assumptions

**** Independent, Identically Distributed
- Population $\omega_1, \omega_2, \ldots$ with $\omega_t \in \Omega$
- $\omega_t \sim P$: individuals $\omega_t \in \Omega$ are drawn from some underlying distribution $P$
- $\bx_t \defn \bx(\omega_t)$ are some features of the $t$-th individual
**** Representative sample
- We observe a sample of $T$ individuals through their features $\bx$
- This sample is uniformly selected at random

e can treat a sample of $T$ individuals as just an 

*** Two variables: conditional expectation
**** The height of different genders

*** Learning from data
    
**** Unsupervised learning
- Given data $x_1, \ldots, x_T$.
- Learn about the data-generating process.
- Example: Estimation, compression, text/image generation  
**** Supervised learning
- Given data $(x_1, y_1), \ldots, (x_T, y_T)$
- Learn about the relationship between $x_t$ and $y_t$.
- Example: Classification, Regression
**** Online learning
- Sequence prediction: At each step $t$, predict $x_{t+1}$ from $x_1, \ldots, x_t$.
- Conditional prediction: At each step $t$, predict $y_{t+1}$ from $x_1, y_1 \ldots, x_t, y_t, \alert{x_{t+1}}$
**** Reinforcement learning
 Learn to act in an *unknown* world through interaction and rewards





*** Models
**** Models as summaries
- They summarise what we can see in the data
- The ultimate model of the data *is* the data
**** Models as predictors
- They make predictions about *out of sample* data
- This requires some *assumptions about the data distribution*.
**** Example models
- A numerical mean
- A linear classifier
- A linear regressor
- A deep neural network
- A Gaussian process
- A large language model

*** Validating models
**** Training data
- Calculations, optimisation
- Data exploration
**** Validation data
- Fine-tuning
- Model selection
**** Test data
- Performance comparison
**** Simulation
- Interactive performance comparison
- White box testing
**** Real-world testing
- Actual performance measurement

*** The simplest model: A mean


* Validating models
*** Robust models of the mean

*** Model selection
- Train/Test/Validate
- Cross-validation
- Simulation

* Course summary

** Course Contents

*** Course Contents
**** Models
- k-Nearest Neighbours.
- Linear models and perceptrons.
- Multi-layer perceptrons (aka deep neural networks).
- Bayesian Networks
**** Algorithms
- (Stochastic) Gradient Descent.
- Bayesian inference.
  
*** Supervised learning
    The general goal is learning a function $f: X \to Y$.
**** Classification
- Input data $x_t \in \Reals$, $y_t \in [m] = \{1, 2, \ldots, m\}$
- Learn a mapping $f$ so that $f(x_t) = y_t$ for unseen data
**** Regression
- Input data $x_t, y_t$
- Learn a mapping $f$ so that $f(x_t) = \E[y_t]$ for unseen data
*** Unsupervised learning
The general goal is learning the data distribution.
**** Compression
- Learn two mappings $c, d$
- $c(x)$ compresses an image $x$ to a small representation $z$.
- $d(z)$ decompresses to an approximate image $\hat{x}$.
**** Density estimation
- Input data $x_1, \ldots, x_T$ from distribution with density $p$
- Problem: Estimate $p$.
**** Clustering
- Input data $x_1, \ldots, x_T$ 
- Assign each data $x_t$. to  cluster label $c_t$.
** Objective functions
*** Supervised learning objectives
- Data $(x_t, y_t)$, $x_t \in X$, $y_t \in Y$, $t \in [T]$.
- i.i.d assumption: $(x_t, y_t) \sim P$ for all $t$.
- Supervised decision rule $\pi(a_t | x_t)$
**** Classification
- Predict the labels correctly, i.e. $a_t = y_t$.
- Have an appropriate confidence level

**** Regression
- Predict the mean correctly
- Have an appropriate variance around the mean
*** Unsupervised learning objectives
- Reconstruct the data well
- Be able to generate data
*** Reinforcement learning objectives
- Maximise total reward

  
** Pitfalls
*** Pitfalls
**** Reproducibility
- Modelling assumptions
- Distribution shift
- Interactions and feedback
**** Fairness
- Implicit biases in training data
- Fair decision rules and meritocracy
**** Privacy
- Accidental data disclosure
- Re-identification risk

* Reading

ISLP Chapter 1
